{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Japanese fake news classification\n",
    "\n",
    "We're going to attempt to classify this dataset. This is a dataset featuring Japanese news articles, from which a part\n",
    "real, some are half fake, and some are entirely fake.\n",
    "\n",
    "0: Original article\n",
    "1: Partially fake\n",
    "2: Completely fake\n",
    "\n",
    "Source: [Japanese fakenews dataset](https://www.kaggle.com/tanreinama/japanese-fakenews-dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\software\\coding projects\\aai\\japanesetextclassification\\venv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "              count        mean         std  min  25%    50%     75%     max\nisfake      13040.0    1.075613    0.796950  0.0  0.0    1.0    2.00     2.0\nnchar_real  13040.0  261.029371  288.257753  0.0  0.0  215.0  407.00  4447.0\nnchar_fake  13040.0  328.273160  354.673240  0.0  0.0  255.0  489.25  2582.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>isfake</th>\n      <td>13040.0</td>\n      <td>1.075613</td>\n      <td>0.796950</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.00</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>nchar_real</th>\n      <td>13040.0</td>\n      <td>261.029371</td>\n      <td>288.257753</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>215.0</td>\n      <td>407.00</td>\n      <td>4447.0</td>\n    </tr>\n    <tr>\n      <th>nchar_fake</th>\n      <td>13040.0</td>\n      <td>328.273160</td>\n      <td>354.673240</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>255.0</td>\n      <td>489.25</td>\n      <td>2582.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sudachipy import tokenizer, dictionary\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for i in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(i, enable=True)\n",
    "\n",
    "df = pd.read_csv(\"fakenews.csv\")\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows/Columns: (13040, 5)\n",
      "Class distribution is: \n",
      "1    4684\n",
      "2    4671\n",
      "0    3685\n",
      "Name: isfake, dtype: int64\n",
      "\n",
      "id            0\n",
      "context       0\n",
      "isfake        0\n",
      "nchar_real    0\n",
      "nchar_fake    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Rows/Columns: {df.shape}')\n",
    "print(f\"Class distribution is: \\n{df['isfake'].value_counts()}\\n\")\n",
    "print(df.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From what we can see the dataset does not contain null values, and it's class distribution is mostly even.\n",
    "\n",
    "Although mostly even, not perfectly even, so while my first thought was accuracy as a metric, i've decided to use the\n",
    "ROC curve instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(df['context'].head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    朝日新聞など各社の報道によれば、宅配便最大手「ヤマト運輸」が日本郵政公社を相手取り、大手コン...\n",
      "1    11月5日の各社報道によると、諫早湾干拓事業は諫早海人（諫早湾の「海」）に囲まれる大洋に位置...\n",
      "2    産経新聞、中日新聞によると、2004年から2005年まで、この大会による3年おきの開催を、2...\n",
      "3    開催地のリオデジャネイロ市に対して、大会期間中のリオデジャネイロオリンピックに関する公式発表...\n",
      "4    毎日新聞・時事通信によると、2006年2月13日には、グッドウィル・グッゲンハイム・アン・ハ...\n",
      "Name: context, dtype: object\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "We can mostly follow the standard NLP cleaning methods, albeit with a catch, it's a character based language, so the\n",
    "traditional packages such as NLTK won't work. That's why we need to find substitutes for the cleaning methods. Let's go\n",
    "over them:\n",
    "\n",
    "I've decided to go with [SudachiPy](https://pypi.org/project/SudachiPy/) for tokenization and stemming\n",
    "[stopwords-ja](https://github.com/stopwords-iso/stopwords-ja/blob/master/stopwords-ja.json) (link to json file) for stopword removal.\n",
    "\n",
    "I decided to not use lemmatization, since there are a lot of words that might _seem_ similar, but are vastly different\n",
    "taking into account their context."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing features\n",
    "\n",
    "Everything besides the actual text and the label class is just noise, so we'll take those out."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "labels = df.pop('isfake')\n",
    "data = df.pop('context')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "\n",
    "We're going to tokenize our sentences. This we'll do with Sudachi, a morphological analyzer. The short version of\n",
    "what that means is that we're splitting our sentences up into pieces, aka tokens. We need a specialized analyzer for\n",
    "this task, because you can't just randomly split it up by character, because then you'd lose context. After tokenization\n",
    "is complete we can do the rest of the preprocessing steps, which we usually apply individually per character."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Special characters\n",
    "\n",
    "We need to also remove some special characters that will be of no use to the classifier, think of stuff such as commas\n",
    "and question marks, etc. We do this via a regex filter. I spent some time looking for the exactly right one online,\n",
    "since I wanted to keep all japanese characters, but remove junk such as symbols. Speaking of symbols for some reason\n",
    "the Japanese have their own version of them for some reason (。、？  notice how they're different?), which is pretty\n",
    "tricky. While you'd want to remove irrelevant characters, you also don't want to remove all symbols, since some are\n",
    "very important to the language (e.g. ー, which lengthens vowels.).\n",
    "\n",
    "The conclusion here is that you'll have to build your own japanese regex filter to suit your exact needs. Definitely\n",
    "check out [this gist](https://gist.github.com/terrancesnyder/1345094) for that.\n",
    "\n",
    "Check out the pre-processing function below for the specific regex i used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming implies reducing the form of a word to its stem. E.g. 食べている -->　食べる, した　--> する. This is useful to\n",
    "reduce the amount of variants of a word which mostly mean the exact same thing (from the perspective of the model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopword removal implies the removal of certain words which appear in an excessive frequency in the language. Common\n",
    "examples for English are \"the\", \"and\" and so forth. These are useful to remove because they lose meaning viewed\n",
    "individually."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朝日新聞など各社の報道によれば、宅配便最大手「ヤマト運輸」が日本郵政公社を相手取り、大手コンビニエンスストア「ローソン」でのサービス提供の差し止めなどを求めていた訴訟で、2006年1月19日、東京地方裁判所でヤマト運輸の請求を棄却する判決が下された。2004年のローソンでの郵便小包サービス「ゆうパック」の受付業務開始に際し、ヤマト運輸は「独占禁止法に違反する不当な廉売」として、日本郵政公社を相手取り、サービス提供の差し止めなどを求めていた。朝日新聞によれば、提訴の内容は、2004年11月のローソンでの「ゆうパック」の受付サービス提供の開始に関連し、租税などの優遇措置を受けている日本郵政公社が、配送料金（運賃）などの有利な取引条件でローソンで「ゆうパック」を開始させたのは、独占禁止法の不当廉売に当たり、ヤマト運輸の利益を侵害されるとして、「ゆうパック」サービス提供の差し止めなどを求めていたもの。朝日新聞によれば、判決内容はヤマト運輸の主張を全面的に否定しており、今後の「ゆうパック」サービスの拡大に弾みがつくものと考えられる。日本郵政公社は、公正妥当な判決とのコメントを出した。一方、ヤマト運輸は、高等裁判所への控訴など、今後の対応については検討するとアナウンスしている。\n",
      "\n",
      "11月5日の各社報道によると、諫早湾干拓事業は諫早海人（諫早湾の「海」）に囲まれる大洋に位置することから、人身売買により、環境問題に加え、環境保護にも関心が向けられた。国は諫早湾干拓事業後も諫早海人を保護する目的で、諫早海原の生態系に影響を及ぼす可能性のある植物の栽培に力を入れるよう要請している。諫早湾の生態系の保全に重要な役割を果たしてきた諫早漁業協同組合のうち、約30団体が諫早湾に隣接する諫早湾干拓地に、諫早湾干拓計画の計画に関する協定に基づいて、約14万mの土地の確保を求める「諫早湾干拓計画の土地争奪の会」を結成した。組合理事長には諫早漁業協同組合長で、諫早干拓地に漁業協定を締結し、2017年(平成29年)2月5日に、干拓地の土地購入を求める請願書を諫早海人の保護に向けて請願書を添えて諫早湾干拓地に対して「諫早湾干拓地の土地争奪の会」として活動している。\n",
      "\n",
      "産経新聞、中日新聞によると、2004年から2005年まで、この大会による3年おきの開催を、2006年から2年連続で実施したことがある。また、2月9日の中日戦（日本ハム対阪神）において、先発投手の投手コーチだった岡田彰布が13日夕方（12時前）に体調不良を訴えたため、14日の1st第2戦（4月2日）では、6－1と大勝し大接戦を演じたが、このことを後に岡田は「岡田のファンが応援してくれなかった」とコメント。この試合から野球振興会は第2戦の開催を断念し、2008年から4月まで、第1戦、第2戦が屋内試合となった。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample(data, limit=3):\n",
    "    \"\"\"\n",
    "    Quick util method to display samples of the sentences\n",
    "    :param data: collection\n",
    "    :param limit: how much samples to show\n",
    "    \"\"\"\n",
    "    for x in range(0, limit, 1):\n",
    "        print(f\"{data[x]}\\n\")\n",
    "\n",
    "\n",
    "sample(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning phase, this may take a few minutes...\n",
      "朝日新聞 など 各社 の 報道 に よる ば 宅配便 最大手 ヤマト 運輸 が 日本郵政 公社 を 相手 取る 大手 コンビニエンスストアローソン で の サービス 提供 の 差し止め など を 求める て いる た 訴訟 で 2006 年 1 月 19 日 東京 地方裁判所 で ヤマト 運輸 の 請求 を 棄却 する 判決 が 下す れる た 2004 年 の ローソン で の 郵便 小包 サービス ゆう パック の 受付 業務 開始 に 際する ヤマト 運輸 は 独占禁止法 に 違反 する 不当 だ 廉売 と する て 日本郵政 公社 を 相手 取る サービス 提供 の 差し止め など を 求める て いる た 朝日新聞 に よる ば 提訴 の 内容 は 2004 年 11 月 の ローソン で の ゆう パック の 受付 サービス 提供 の 開始 に 関連 する 租税 など の 優遇措置 を 受ける て いる 日本郵政 公社 が 配送 料金 運賃 など の 有利 だ 取引 条件 で ローソン で ゆう パック を 開始 する せる た の は 独占禁止法 の 不当廉売 に 当たる ヤマト 運輸 の 利益 を 侵害 する れる と する て ゆう パック サービス 提供 の 差し止め など を 求める て いる た もの 朝日新聞 に よる ば 判決 内容 は ヤマト 運輸 の 主張 を 全面的 だ 否定 する て おる 今後 の ゆう パック サービス の 拡大 に 弾み が つく もの と 考える られる 日本郵政 公社 は 公正 妥当 だ 判決 と の コメント を 出す た 一方 ヤマト 運輸 は 高等裁判所 へ の 控訴 など 今後 の 対応 に つく て は 検討 する と アナウンス する て いる\n",
      "\n",
      "11 月 5 日 の 各社 報道 に よる と 諫早湾 干拓 事業 は 諫早 海人 諫早湾 の 海 に 囲む れる 大洋 に 位置 する こと から 人身 売買 に よる 環境 問題 に 加える 環境 保護 に も 関心 が 向ける られる た 国 は 諫早湾 干拓 事業 後 も 諫早 海人 を 保護 する 目的 で 諫早 海原 の 生態系 に 影響 を 及ぼす 可能性 の ある 植物 の 栽培 に 力 を 入れる よう 要請 する て いる 諫早湾 の 生態系 の 保全 に 重要 だ 役割 を 果たす て くる た 諫早 漁業協同組合 の うち 約 30 団体 が 諫早湾 に 隣接 する 諫早湾 干拓地 に 諫早湾 干拓 計画 の 計画 に 関する 協定 に 基づく て 約 14万 m の 土地 の 確保 を 求める 諫早湾 干拓 計画 の 土地 争奪 の 会 を 結成 する た 組合 理事長 に は 諫早 漁業協同組合 長 だ 諫早 干拓地 に 漁業 協定 を 締結 する 2017 年 平成 29 年 2 月 5 日 に 干拓地 の 土地 購入 を 求める 請願書 を 諫早 海人 の 保護 に 向ける て 請願書 を 添える て 諫早湾 干拓地 に 対する て 諫早湾 干拓地 の 土地 争奪 の 会 と する て 活動 する て いる\n",
      "\n",
      "産経 新聞 中日 新聞 に よる と 2004 年 から 2005 年 まで この 大会 に よる 3 年 おき の 開催 を 2006 年 から 2 年 連続 で 実施 する た こと が ある また 2 月 9 日 の 中日 戦 日本ハム 対 阪神 に おく て 先発 投手 の 投手 コーチ だ た 岡田 彰布 が 13 日 夕方 12 時 前 に 体調不良 を 訴える た ため 14 日 の 1 ST 第 2 戦 4 月 2 日 で は 61 と 大勝 する 大 接戦 を 演ずる た が この こと を 後 に 岡田 は 岡田 の ファン が 応援 する て くれる ない た と コメント この 試合 から 野球 振興会 は 第 2 戦 の 開催 を 断念 する 2008 年 から 4 月 まで 第 1 戦 第 2 戦 が 屋内 試合 と なる た\n",
      "\n",
      "開催地 の リオデジャネイロ 市 に 対する て 大会 期間中 の リオデジャネイロ オリンピック に 関する 公式 発表 は ある た もの の リオデジャネイロ オリンピック 開催 に 向ける て 大会 会場 の 準備 が 整える られる ない 現状 が ある た その 結果 オリンピック の 公式 イベント と する て リオデジャネイロ の ホテル や 各種 ショップ で は リオデジャネイロ オリンピック 記念 パーティーリオデジャネイロオリンピック 応援 パーティー が 13 日 まで に オープン する 中 13 日 以降 に は リオデジャネイロ の ホテル で リオデジャネイロ オリンピック 応援 パーティー 5 日 から は リオデジャネイロ オリンピック の シンボル プロモーション が 開催 する れる よう だ なる た その 後 リオデジャネイロ 市 に は リオデジャネイロ オリンピック 公式 式典 が 18 日 開催 する れる リオデジャネイロ オリンピック 実施 に つく て の 公式 発表 が ある た が 19 日 の 大会 組織 委員会 は 大会 直前 に リオデジャネイロ 市長 から リオデジャネイロ 市 と 1 か月 の 予定 の リオデジャネイロ オリンピック 開催 中止 を 理由 の 1 つ に 掲げる こと を 検討 する て いる た と 発表 する た リオデジャネイロ オリンピック の 公式 試合 と なる た リオデジャネイロ 以外 の 大会 に も リオデジャネイロ オリンピック を テーマ と する た イベント が 多数 設置 する れる よう だ なる リオデジャネイロ オリンピック が 開催 する れる 2020 年 は その 第 1 の 都市 と いう こと で リオデジャネイロ オリンピック 関係者 や リオデジャネイロ オリンピック 参加 の アスリート ら が オリンピック イベント を 通す て リオデジャネイロ へ 行く の も 悪い ない と 思う れる て いる た\n",
      "\n",
      "毎日 新聞 時事 通信 に よる と 2006 年 2 月 13 日 に は グッドウィルグッゲンハイムアンハルク を 経営 する ジャーナリスト フリー ライター ジャーナリスト および 日本 政府 の 幹部 から 派遣 事業 の 管理 体制 が とても 曖昧 だ なる 可能性 が ある 派遣 事業 の 管理 体制 を 整える ない ば 派遣 事業 を 行う 社員 が 不当 だ 賃金 を 上げる られる 恐れ が ある など と 非難 が あがる た 2009 年 8 月 29 日 の NHK 総合 テレビジョンクロ ちゃん で この よう だ 事態 に 関する グッドウィル は 全く 悪 印象 が 付く ない もの だ は ない と 主張 する て いる た\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pre_process_text(text, stopwords, tokenizer_obj):\n",
    "    \"\"\"\n",
    "    Cleans the text of unnecessary features\n",
    "    :param text: sentence to clean\n",
    "    :param stopwords: list of very commonly used words\n",
    "    :param tokenizer_obj: SudachiPy tokenizer object\n",
    "    :return: cleaned string\n",
    "    \"\"\"\n",
    "    # One by one: (kanji), (hiragana and katakana), (western alphabet and numbers), (western alphabet and numbers\n",
    "    # except it's the off-looking japanese version), and unicode flag\n",
    "    pattern = re.compile(r'([一-龯]+)|([ぁ-んァ-ン]+)|([a-zA-Z0-9]+)|([ａ-ｚＡ-Ｚ０-９]+)|[ー+]', re.UNICODE)\n",
    "    text = ''.join([x.group() for x in re.finditer(pattern, text.lower().strip())])\n",
    "\n",
    "    # Tokenizes, converts to dictionary form, and doesn't add it if it's in the stopword list\n",
    "    lst_text = [m.dictionary_form() for m in tokenizer_obj.tokenize(text, mode) if m not in stopwords]\n",
    "\n",
    "    # Rejoin tokenized string\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "lst_stopwords = requests.get(\n",
    "    'https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/master/stopwords-ja.json').json()\n",
    "\n",
    "print(\"Starting cleaning phase, this may take a few minutes...\")\n",
    "data = [pre_process_text(i, lst_stopwords, tokenizer_obj) for i in data]\n",
    "sample(data, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLP technique\n",
    "\n",
    "Now we need to choose a technique to actually process this text, because after all, our neural network only accepts\n",
    "numbers, not japanese characters. Some examples of the current most popular techniques are Bag of Words, TF_IDF Scheme,\n",
    "BERT, word2vec.\n",
    "\n",
    "I've decided to use word2vec here, which maps words and/or phrases to vectors. word2vec comes with 2 options, Skip Gram\n",
    "and Continuous Bag of Words, which are essentially mirrored versions of each other. CBOW is trained to predict a single\n",
    "word from multiple context words, SG is trained to predict multiple words from a single context word.\n",
    "\n",
    "I used [this pre-trained skip-gram embedding](https://github.com/singletongue/WikiEntVec/releases) trained on wikipedia articles,\n",
    "which i got from [this comparison of Japanese embeddings](https://blog.hoxo-m.com/entry/2020/02/20/090000) since i\n",
    "felt like it fit with the vocabulary of our news articles.\n",
    "\n",
    "To load our embeddings i used Gensim, since it's the easiest to use."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embedding into Gensim, this may take a while...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading word embedding into Gensim, this may take a while...\")\n",
    "w2vec_model = KeyedVectors.load_word2vec_format(\"jawiki.all_vectors.300d.txt\", binary=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example of vocabulary in the w2vec model\n",
    "vocabulary = w2vec_model.index_to_key\n",
    "print(vocabulary[10:20], \"\\n\")\n",
    "\n",
    "# Example of how to access vector of a word\n",
    "print(w2vec_model['から'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weights = w2vec_model.vectors\n",
    "vocabulary_length = weights.shape[0]\n",
    "print(f\"Imported word2vec model has {vocabulary_length} entries\")\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=weights.shape[0],\n",
    "    output_dim=weights.shape[1],\n",
    "    weights=[weights],\n",
    "    trainable=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(data)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(data)\n",
    "print(encoded_docs[0])\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, train_size=0.85, stratify=labels, shuffle=True)\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_pred = model.predict(X_test)\n",
    "nn_pred =  np.where(nn_pred > 0.5, 1, 0 )\n",
    "ac = accuracy_score(y_test, nn_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Package wise, we still have a long way to go. The Japanese equivalents of the NLP packages are more underveloped,\n",
    "less reliable, and generally slower too. This can be mostly attributed to the language being difficult, but it's\n",
    "something you have to keep in mind while working with Japanese text. Maybe in a few years the tools will be at a good\n",
    "level.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "[Preprocessing Methods and Tools in Modelling Japanese for Text Classification](https://www.researchgate.net/publication/335337209_Preprocessing_Methods_and_Tools_in_Modelling_Japanese_for_Text_Classification)\n",
    "Paper detailing tools for Japanese NLP analysis. A few years old, but still relevant as of current date.\n",
    "\n",
    "[nlp-recipes-ja](https://github.com/upura/nlp-recipes-ja)\n",
    "Github repository containing a ton of samples for Japanese text analysis in Python.\n",
    "\n",
    "[Migrating from Gensim 3.0 to 4.0](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4)\n",
    "Gensim recently upgraded to 4.0 release, which contains code-breaking API changes, and most recent material on gensim is\n",
    "still using older versions.\n",
    "\n",
    "[Gensim Keras embedding layer example](https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow)\n",
    "How to create a Keras embedding layer with a Gensim model\n",
    "\n",
    "[Using pretrained gensim Word2vec embedding in keras](https://stackoverflow.com/q/60082554/7174982)\n",
    "Helpful answer on how to encode the text sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}