{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Japanese fake news classification\n",
    "\n",
    "We're going to attempt to classify this dataset. This is a dataset featuring Japanese news articles, from which a part\n",
    "real, some are half fake, and some are entirely fake.\n",
    "\n",
    "0: Original article\n",
    "1: Partially fake\n",
    "2: Completely fake\n",
    "\n",
    "Source: [Japanese fakenews dataset](https://www.kaggle.com/tanreinama/japanese-fakenews-dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              count        mean         std  min  25%    50%     75%     max\nisfake      13040.0    1.075613    0.796950  0.0  0.0    1.0    2.00     2.0\nnchar_real  13040.0  261.029371  288.257753  0.0  0.0  215.0  407.00  4447.0\nnchar_fake  13040.0  328.273160  354.673240  0.0  0.0  255.0  489.25  2582.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>isfake</th>\n      <td>13040.0</td>\n      <td>1.075613</td>\n      <td>0.796950</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.00</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>nchar_real</th>\n      <td>13040.0</td>\n      <td>261.029371</td>\n      <td>288.257753</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>215.0</td>\n      <td>407.00</td>\n      <td>4447.0</td>\n    </tr>\n    <tr>\n      <th>nchar_fake</th>\n      <td>13040.0</td>\n      <td>328.273160</td>\n      <td>354.673240</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>255.0</td>\n      <td>489.25</td>\n      <td>2582.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"fakenews.csv\")\n",
    "\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows/Columns: (13040, 5)\n",
      "Class distribution is: \n",
      "1    4684\n",
      "2    4671\n",
      "0    3685\n",
      "Name: isfake, dtype: int64\n",
      "\n",
      "id            0\n",
      "context       0\n",
      "isfake        0\n",
      "nchar_real    0\n",
      "nchar_fake    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Rows/Columns: {df.shape}')\n",
    "print(f\"Class distribution is: \\n{df['isfake'].value_counts()}\\n\")\n",
    "print(df.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From what we can see the dataset does not contain null values, and it's class distribution is mostly even.\n",
    "\n",
    "Although mostly even, not perfectly even, so while my first thought was accuracy as a metric, i've decided to use the\n",
    "ROC curve instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(df['context'].head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    朝日新聞など各社の報道によれば、宅配便最大手「ヤマト運輸」が日本郵政公社を相手取り、大手コン...\n",
      "1    11月5日の各社報道によると、諫早湾干拓事業は諫早海人（諫早湾の「海」）に囲まれる大洋に位置...\n",
      "2    産経新聞、中日新聞によると、2004年から2005年まで、この大会による3年おきの開催を、2...\n",
      "3    開催地のリオデジャネイロ市に対して、大会期間中のリオデジャネイロオリンピックに関する公式発表...\n",
      "4    毎日新聞・時事通信によると、2006年2月13日には、グッドウィル・グッゲンハイム・アン・ハ...\n",
      "Name: context, dtype: object\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "We can mostly follow the standard NLP cleaning methods, albeit with a catch, it's a character based language, so the\n",
    "traditional packages such as NLTK won't work. That's why we need to find substitutes for the cleaning methods. Let's go\n",
    "over them:\n",
    "\n",
    "I've decided to go with [SudachiPy](https://pypi.org/project/SudachiPy/) for tokenization,\n",
    "[JapaneseStemmer](https://github.com/asherperkins11/Japanese-stemmer) (you'll have to copy the file locally) for stemming and\n",
    "[stopwords-ja](https://github.com/stopwords-iso/stopwords-ja/blob/master/stopwords-ja.json) (link to json file) for stopword removal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing features\n",
    "\n",
    "Everything besides the actual text and the label class is just noise, so we'll take those out."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "labels = df.pop('isfake')\n",
    "data = df.pop('context')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "\n",
    "We're going to tokenize our sentences. This we'll do with Sudachi, a morphological analyzer. The short version of\n",
    "what that means is that we're splitting our sentences up into pieces, aka tokens. We need a specialized analyzer for\n",
    "this task, because you can't just randomly split it up by character, because then you'd lose context. After tokenization\n",
    "is complete we can do the rest of the preprocessing steps, which we usually apply individually per character."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Package wise, we still have a long way to go. The Japanese equivalents of the NLP packages are more underveloped,\n",
    "less reliable, and generally slower too. This can be mostly attributed to the language being difficult, but it's\n",
    "something you have to keep in mind while working with Japanese text. Maybe in a few years the tools will be at a good\n",
    "level.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "[Preprocessing Methods and Tools in Modelling Japanese for Text Classification](https://www.researchgate.net/publication/335337209_Preprocessing_Methods_and_Tools_in_Modelling_Japanese_for_Text_Classification)\n",
    "Paper detailing tools for Japanese NLP analysis. A few years old, but still relevant as of current date.\n",
    "\n",
    "[nlp-recipes-ja](https://github.com/upura/nlp-recipes-ja)\n",
    "Github repository containing a ton of samples for Japanese text analysis in Python.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}