{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Japanese fake news classification\n",
    "\n",
    "We're going to attempt to classify this dataset. This is a dataset featuring Japanese news articles, from which a part\n",
    "real, some are half fake, and some are entirely fake.\n",
    "\n",
    "0: Original article\n",
    "1: Partially fake\n",
    "2: Completely fake\n",
    "\n",
    "Source: [Japanese fakenews dataset](https://www.kaggle.com/tanreinama/japanese-fakenews-dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              count        mean         std  min  25%    50%     75%     max\nisfake      13040.0    1.075613    0.796950  0.0  0.0    1.0    2.00     2.0\nnchar_real  13040.0  261.029371  288.257753  0.0  0.0  215.0  407.00  4447.0\nnchar_fake  13040.0  328.273160  354.673240  0.0  0.0  255.0  489.25  2582.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>isfake</th>\n      <td>13040.0</td>\n      <td>1.075613</td>\n      <td>0.796950</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.00</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>nchar_real</th>\n      <td>13040.0</td>\n      <td>261.029371</td>\n      <td>288.257753</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>215.0</td>\n      <td>407.00</td>\n      <td>4447.0</td>\n    </tr>\n    <tr>\n      <th>nchar_fake</th>\n      <td>13040.0</td>\n      <td>328.273160</td>\n      <td>354.673240</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>255.0</td>\n      <td>489.25</td>\n      <td>2582.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"fakenews.csv\")\n",
    "\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows/Columns: (13040, 5)\n",
      "Class distribution is: \n",
      "1    4684\n",
      "2    4671\n",
      "0    3685\n",
      "Name: isfake, dtype: int64\n",
      "\n",
      "id            0\n",
      "context       0\n",
      "isfake        0\n",
      "nchar_real    0\n",
      "nchar_fake    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Rows/Columns: {df.shape}')\n",
    "print(f\"Class distribution is: \\n{df['isfake'].value_counts()}\\n\")\n",
    "print(df.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From what we can see the dataset does not contain null values, and it's class distribution is mostly even.\n",
    "\n",
    "Although mostly even, not perfectly even, so while my first thought was accuracy as a metric, i've decided to use the\n",
    "ROC curve instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(df['context'].head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    朝日新聞など各社の報道によれば、宅配便最大手「ヤマト運輸」が日本郵政公社を相手取り、大手コン...\n",
      "1    11月5日の各社報道によると、諫早湾干拓事業は諫早海人（諫早湾の「海」）に囲まれる大洋に位置...\n",
      "2    産経新聞、中日新聞によると、2004年から2005年まで、この大会による3年おきの開催を、2...\n",
      "3    開催地のリオデジャネイロ市に対して、大会期間中のリオデジャネイロオリンピックに関する公式発表...\n",
      "4    毎日新聞・時事通信によると、2006年2月13日には、グッドウィル・グッゲンハイム・アン・ハ...\n",
      "Name: context, dtype: object\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "We can mostly follow the standard NLP cleaning methods, albeit with a catch, it's a character based language, so the\n",
    "traditional packages such as NLTK won't work. That's why we need to find substitutes for the cleaning methods. Let's go\n",
    "over them:\n",
    "\n",
    "I've decided to go with [SudachiPy](https://pypi.org/project/SudachiPy/) for tokenization,\n",
    "[JapaneseStemmer](https://github.com/asherperkins11/Japanese-stemmer) (you'll have to copy the file locally) for stemming and\n",
    "[stopwords-ja](https://github.com/stopwords-iso/stopwords-ja/blob/master/stopwords-ja.json) (link to json file) for stopword removal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing features\n",
    "\n",
    "Everything besides the actual text and the label class is just noise, so we'll take those out."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "labels = df.pop('isfake')\n",
    "data = df.pop('context')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "\n",
    "We're going to tokenize our sentences. This we'll do with Sudachi, a morphological analyzer. The short version of\n",
    "what that means is that we're splitting our sentences up into pieces, aka tokens. We need a specialized analyzer for\n",
    "this task, because you can't just randomly split it up by character, because then you'd lose context. After tokenization\n",
    "is complete we can do the rest of the preprocessing steps, which we usually apply individually per character."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Special characters\n",
    "\n",
    "We need to also remove some special characters that will be of no use to the classifier, think of stuff such as commas\n",
    "and question marks, etc. We do this via a regex filter. I spent some time looking for the exactly right one online,\n",
    "since I wanted to keep all japanese characters, english text, and numbers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming implies reducing the form of a word to its stem. E.g. 食べている -->　食べる, した　--> する. This is useful to\n",
    "reduce the amount of variants of a word which mostly mean the exact same thing (from the perspective of the model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopword removal implies the removal of certain words which appear in an excessive frequency in the language. Common\n",
    "examples for English are \"the\", \"and\" and so forth. These are useful to remove because they are so often used that they\n",
    "lose meaning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朝日新聞など各社の報道によれば、宅配便最大手「ヤマト運輸」が日本郵政公社を相手取り、大手コンビニエンスストア「ローソン」でのサービス提供の差し止めなどを求めていた訴訟で、2006年1月19日、東京地方裁判所でヤマト運輸の請求を棄却する判決が下された。2004年のローソンでの郵便小包サービス「ゆうパック」の受付業務開始に際し、ヤマト運輸は「独占禁止法に違反する不当な廉売」として、日本郵政公社を相手取り、サービス提供の差し止めなどを求めていた。朝日新聞によれば、提訴の内容は、2004年11月のローソンでの「ゆうパック」の受付サービス提供の開始に関連し、租税などの優遇措置を受けている日本郵政公社が、配送料金（運賃）などの有利な取引条件でローソンで「ゆうパック」を開始させたのは、独占禁止法の不当廉売に当たり、ヤマト運輸の利益を侵害されるとして、「ゆうパック」サービス提供の差し止めなどを求めていたもの。朝日新聞によれば、判決内容はヤマト運輸の主張を全面的に否定しており、今後の「ゆうパック」サービスの拡大に弾みがつくものと考えられる。日本郵政公社は、公正妥当な判決とのコメントを出した。一方、ヤマト運輸は、高等裁判所への控訴など、今後の対応については検討するとアナウンスしている。\n",
      "\n",
      "\n",
      "11月5日の各社報道によると、諫早湾干拓事業は諫早海人（諫早湾の「海」）に囲まれる大洋に位置することから、人身売買により、環境問題に加え、環境保護にも関心が向けられた。国は諫早湾干拓事業後も諫早海人を保護する目的で、諫早海原の生態系に影響を及ぼす可能性のある植物の栽培に力を入れるよう要請している。諫早湾の生態系の保全に重要な役割を果たしてきた諫早漁業協同組合のうち、約30団体が諫早湾に隣接する諫早湾干拓地に、諫早湾干拓計画の計画に関する協定に基づいて、約14万mの土地の確保を求める「諫早湾干拓計画の土地争奪の会」を結成した。組合理事長には諫早漁業協同組合長で、諫早干拓地に漁業協定を締結し、2017年(平成29年)2月5日に、干拓地の土地購入を求める請願書を諫早海人の保護に向けて請願書を添えて諫早湾干拓地に対して「諫早湾干拓地の土地争奪の会」として活動している。\n",
      "\n",
      "\n",
      "産経新聞、中日新聞によると、2004年から2005年まで、この大会による3年おきの開催を、2006年から2年連続で実施したことがある。また、2月9日の中日戦（日本ハム対阪神）において、先発投手の投手コーチだった岡田彰布が13日夕方（12時前）に体調不良を訴えたため、14日の1st第2戦（4月2日）では、6－1と大勝し大接戦を演じたが、このことを後に岡田は「岡田のファンが応援してくれなかった」とコメント。この試合から野球振興会は第2戦の開催を断念し、2008年から4月まで、第1戦、第2戦が屋内試合となった。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample(data, limit=3):\n",
    "    \"\"\"\n",
    "    Quick util method to display samples of the sentences\n",
    "    :param data: collection\n",
    "    :param limit: how much samples to show\n",
    "    \"\"\"\n",
    "    for x in range(0, limit, 1):\n",
    "        print(f\"{data[x]}\\n\\n\")\n",
    "\n",
    "sample(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Package wise, we still have a long way to go. The Japanese equivalents of the NLP packages are more underveloped,\n",
    "less reliable, and generally slower too. This can be mostly attributed to the language being difficult, but it's\n",
    "something you have to keep in mind while working with Japanese text. Maybe in a few years the tools will be at a good\n",
    "level.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "[Preprocessing Methods and Tools in Modelling Japanese for Text Classification](https://www.researchgate.net/publication/335337209_Preprocessing_Methods_and_Tools_in_Modelling_Japanese_for_Text_Classification)\n",
    "Paper detailing tools for Japanese NLP analysis. A few years old, but still relevant as of current date.\n",
    "\n",
    "[nlp-recipes-ja](https://github.com/upura/nlp-recipes-ja)\n",
    "Github repository containing a ton of samples for Japanese text analysis in Python.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}